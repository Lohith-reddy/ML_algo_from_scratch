{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/boston.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax',\n",
       "       'ptratio', 'b', 'lstat', 'medv'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[\"crim\",\"age\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check multi-regression with OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using OLS method to find the solution\n",
      "The model has been fitted successfully with OLS\n",
      "The coefficients are:  [29.80066701 -0.31181577 -0.08955328]\n",
      "The mean squared error is:  66.13782635525362\n",
      "The root mean squared error is:  8.132516606515699\n",
      "The r2 score is:  0.2165579947742563\n",
      "The adjusted r2 score is:  0.21187607044023793\n",
      "The model has been fitted successfully with r squared: 0.2165579947742563\n"
     ]
    }
   ],
   "source": [
    "import main\n",
    "own_model = main.LinReg()\n",
    "own_model.fit(x=X, y=df[\"medv\"],verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5006662579253316e-11"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(own_model.residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "sk_model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted = sk_model.fit(X,df[\"medv\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.31181577 -0.08955328]\n",
      "29.800667010997643\n",
      "0.21655799477425608\n"
     ]
    }
   ],
   "source": [
    "print(fitted.coef_)\n",
    "print(fitted.intercept_)\n",
    "print(own_model.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing for single independent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using OLS method to find the solution\n",
      "The model has been fitted successfully with OLS\n",
      "The coefficients are:  [24.03310617 -0.41519028]\n",
      "The mean squared error is:  71.69073588196659\n",
      "The root mean squared error is:  8.467038200100824\n",
      "The r2 score is:  0.15078046904975717\n",
      "The adjusted r2 score is:  0.14740385063643613\n",
      "The model has been fitted successfully\n"
     ]
    }
   ],
   "source": [
    "import main\n",
    "own_model_1d = main.LinReg()\n",
    "own_model_1d.fit(x=X[\"crim\"], y=df[\"medv\"],verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.41519028]\n",
      "24.03310617412388\n",
      "0.15078046904975717\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "sk_model_1d = LinearRegression()\n",
    "fitted_1d = sk_model_1d.fit(X[[\"crim\"]],df[\"medv\"])\n",
    "\n",
    "print(fitted_1d.coef_)\n",
    "print(fitted_1d.intercept_)\n",
    "print(sk_model_1d.score(X[[\"crim\"]],df[\"medv\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking with method \"GD\" for single variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import helper\n",
    "\n",
    "\n",
    "class LinReg:\n",
    "    \"\"\"\n",
    "    This class implements the linear regression model\n",
    "    Fit the model using .fit() method\n",
    "    Predict the target variable using .predict() method\n",
    "\n",
    "\n",
    "\n",
    "    After fitting the model, the following attributes are available:\n",
    "    self.beta: the coefficients of the model\n",
    "    self.residuals: the residuals of the model\n",
    "    self.mse: the mean squared error of the model\n",
    "    self.rmse: the root mean squared error of the model\n",
    "    self.r2: the r2 score of the model\n",
    "    self.r2_adj: the adjusted r2 score of the model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.y = None\n",
    "        self.x = None\n",
    "        self.beta = None\n",
    "        self.residuals = None\n",
    "        self.mse = None\n",
    "        self.rmse = None\n",
    "        self.r2 = None\n",
    "        self.r2_adj = None\n",
    "        self.y_hat = None\n",
    "        self.max_alpha = None\n",
    "        self.verbose = None\n",
    "        self.max_iter = None\n",
    "        self.min_alpha = None\n",
    "        self.history = {}\n",
    "        self.model_method = None\n",
    "        self.alpha = None\n",
    "        self.diff_w = None\n",
    "        self.xtx = None\n",
    "        self.xtx_inv = None\n",
    "        self.xty = None\n",
    "        self.intercept = None\n",
    "        self.y_mean = None\n",
    "        # Add metaclass to return warning when user tries to access these variables before fitting the model\n",
    "        return\n",
    "\n",
    "    def fit(self, x, y, intercept=False, verbose=False, method=\"OLS\", alpha=0.01, max_iter=100):\n",
    "        \"\"\"\n",
    "        :param x: independent variables\n",
    "        :param y: target variable\n",
    "        :param method: the method to fit the model, OLS or GD\n",
    "                       Use GD for large data, function throws an error,\n",
    "                       if OLS is used on data with size > 50000\n",
    "        :param intercept: whether the intercept is pre-defined or not\n",
    "        :param verbose: Boolean, whether to print the summary of the model or not\n",
    "        :param alpha: learning rate for gradient descent - only applicable when method = GD\n",
    "        :param max_iter: maximum number of iterations - only applicable when method = GD\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # add the intercept term to x\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.verbose = verbose\n",
    "        self.max_iter = max_iter\n",
    "        self.alpha = alpha\n",
    "        self.intercept = intercept\n",
    "        self.method = method\n",
    "        self.x = helper.check_if_oned(\n",
    "            self.x)  # check if the independent variable is one dimensional, if so, reshapes it.\n",
    "        if self.method == \"OLS\":\n",
    "            if self.x.size < 50000:\n",
    "                return self.fit_OLS(x=self.x, y=self.y, verbose=self.verbose)\n",
    "            else:\n",
    "                raise AssertionError(\"The data is too large for OLS method. Consider using gradient descent method\")\n",
    "        elif self.method == \"GD\":\n",
    "            if self.verbose:\n",
    "                print(\"Using gradient descent method to find the solution\")\n",
    "            return self.fit_GD(self.x, self.y)\n",
    "        elif self.method == \"MLE\":\n",
    "            if self.verbose:\n",
    "                print(\"Initializing\")\n",
    "            return self.fit_MLE(self.x, self.y)\n",
    "\n",
    "    def fit_GD(self, x, y, verbose=False, alpha=0.01, max_iter=100, intercept=False, max_alpha=None,\n",
    "               min_alpha=None):\n",
    "        \"\"\"\n",
    "        calculates the coefficients using gradient descent method.\n",
    "\n",
    "        :param x:\n",
    "        :param y:\n",
    "        :param verbose:\n",
    "        :param alpha:\n",
    "        :param max_iter:\n",
    "        :param intercept:\n",
    "        :param max_alpha:\n",
    "        :param min_alpha:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # re-initialising self.x in case user uses fit_gd() directly\n",
    "        self.x = x\n",
    "        self.verbose = verbose\n",
    "        self.alpha = alpha\n",
    "        self.max_iter = max_iter\n",
    "        self.max_alpha = max_alpha\n",
    "        self.min_alpha = min_alpha\n",
    "        self.history = {}\n",
    "        self.model_method = \"GD\"\n",
    "        self.x = helper.check_if_oned(self.x)\n",
    "        self.y = helper.check_if_oned(self.y)\n",
    "        if not intercept:\n",
    "            self.x = helper.add_intercept(self.x)\n",
    "        if self.verbose:\n",
    "            print(\"Using gradient descent method to find the solution\")\n",
    "        \n",
    "        self.beta = np.ones((self.x.shape[1],1))\n",
    "        \n",
    "        for i in range(self.max_iter):\n",
    "\n",
    "            \"\"\"if self.max_alpha is not None:\n",
    "                if self.max_iter > 0.8 * self.max_iter:\n",
    "                    self.alpha = self.min_alpha\n",
    "                else:\n",
    "                    self.alpha = self.max_alpha\"\"\"\n",
    "            \n",
    "            self.y_hat = np.dot(self.x, self.beta)\n",
    "            self.residuals = self.y - self.y_hat\n",
    "            self.diff_w = (-1 / len(self.x)) * np.dot(self.x.T, self.residuals)\n",
    "            self.beta = self.beta - self.alpha * self.diff_w\n",
    "            # bias is also updated. The intercept allows automatic calculation\n",
    "            #self.history.update(self.cal_metrics())\n",
    "\n",
    "        if self.verbose:\n",
    "            self.cal_metrics()\n",
    "            self.summary()\n",
    "            print(f\"The model has been fitted successfully with r squared: {self.r2}\")\n",
    "            return self.r2_adj\n",
    "        else:\n",
    "            print(f\"{self.r2}\")\n",
    "            return self.r2_adj\n",
    "\n",
    "    def fit_MLE(self, x, y, verbose=False, intercept=False):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.verbose = verbose\n",
    "        self.x = helper.check_if_oned(self.x)\n",
    "        print(\"Using MLE method to find the solution\")\n",
    "        self.model_method = \"MLE\"\n",
    "        # calculating the beta\n",
    "        # Calculate the mean of X and y\n",
    "        if np.isnan(self.x).any():\n",
    "            raise ValueError(\"X contains NaN values. Please check your data\")\n",
    "        self.x_mean = np.mean(self.x)\n",
    "        self.y_mean = np.mean(self.y)\n",
    "        print(self.x_mean, self.y_mean)\n",
    "        # Calculate the slope (coefficient)\n",
    "        numerator = np.sum((self.x - self.x_mean) * (self.y - self.y_mean))\n",
    "        denominator = np.sum((self.x - self.x_mean) ** 2)\n",
    "        # if denominator == 0:\n",
    "        #    raise ValueError(\"Denominator is zero. Check your data.\")\n",
    "        self.slope = (numerator / (denominator))\n",
    "        print(self.slope)\n",
    "        # Calculate the intercept\n",
    "        self.bias = self.y_mean - (self.slope * self.x_mean)\n",
    "        self.beta = [self.bias].extend(self.slope)\n",
    "        self.cal_metrics()\n",
    "        if self.verbose:\n",
    "            self.summary()\n",
    "            return print(f\"The model has been fitted successfully with r squared: {self.r2}\")\n",
    "        return self.r2_adj\n",
    "\n",
    "    def fit_OLS(self, x, y, verbose=False, intercept=False):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.verbose = verbose\n",
    "        self.x = helper.check_if_oned(self.x)\n",
    "        if not self.intercept:\n",
    "            self.x = helper.add_intercept(self.x)\n",
    "        print(\"Using OLS method to find the solution\")\n",
    "        self.model_method = \"OLS\"\n",
    "        # calculating the beta\n",
    "        xtx = np.dot(self.x.T, self.x)\n",
    "        xtx_inv = np.linalg.inv(xtx)\n",
    "        xty = np.dot(self.x.T, self.y)\n",
    "        self.beta = np.dot(xtx_inv, xty)\n",
    "        self.cal_metrics()\n",
    "        if self.verbose:\n",
    "            self.summary()\n",
    "            return print(f\"The model has been fitted successfully with r squared: {self.r2}\")\n",
    "        return self.r2_adj\n",
    "\n",
    "    def predict(self, x):\n",
    "        # sample prediction\n",
    "        pred = np.dot(self.x, self.beta)\n",
    "        return pred\n",
    "\n",
    "    def summary(self):\n",
    "        print(f\"The model has been fitted successfully with {self.model_method}\")\n",
    "        print(\"The coefficients are: \", self.beta)\n",
    "        # print(\"The residuals are: \", self.residuals)\n",
    "        print(\"The mean squared error is: \", self.mse)\n",
    "        print(\"The root mean squared error is: \", self.rmse)\n",
    "        print(\"The r2 score is: \", self.r2)\n",
    "        print(\"The adjusted r2 score is: \", self.r2_adj)\n",
    "        return\n",
    "\n",
    "    def cal_metrics(self):\n",
    "        if self.y_hat is None:\n",
    "            self.y_hat = np.dot(self.x, self.beta)\n",
    "        # calculating the residuals\n",
    "        if self.residuals is None:\n",
    "            self.residuals = self.y - self.y_hat\n",
    "        # calculating the mean squared error\n",
    "        self.mse = np.mean(self.residuals ** 2)\n",
    "        # calculating the root mean squared error\n",
    "        self.rmse = np.sqrt(self.mse)\n",
    "\n",
    "        # calculating the r2 score\n",
    "        if self.y_mean is None:\n",
    "            self.y_mean = np.mean(self.y)\n",
    "        self.y_pred_mean = np.mean(self.y_hat)\n",
    "        self.tss = np.sum((self.y - self.y_mean) ** 2)\n",
    "        self.rss = np.sum((self.y - self.y_hat) ** 2)\n",
    "        self.r2 = 1 - (self.rss / self.tss)\n",
    "        # calculating the adjusted r2 score\n",
    "        self.r2_adj = 1 - (1 - self.r2) * (len(self.y) - 1) / (len(self.y) - self.x.shape[1] - 1)\n",
    "\n",
    "        return {'mse': self.mse, 'rmse': self.rmse, 'r2': self.r2, 'r2_adj': self.r2_adj}\n",
    "\n",
    "    def get_history(self):\n",
    "        if self.history == {}:\n",
    "            raise AssertionError(\"The model has not been fitted yet or the method is not GD\")\n",
    "        return self.history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using gradient descent method to find the solution\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#import main\n",
    "own_model_gd = LinReg()\n",
    "own_model_gd.fit(x=X, y=df[\"medv\"],verbose = True, method = \"GD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = own_model_gd.cal_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mse': inf, 'rmse': inf, 'r2': medv   -inf\n",
      "dtype: float64, 'r2_adj': medv   -inf\n",
      "dtype: float64}\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
