{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/boston.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax',\n",
       "       'ptratio', 'b', 'lstat', 'medv'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[\"crim\",\"age\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check multi-regression with OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using OLS method to find the solution\n",
      "The model has been fitted successfully with OLS\n",
      "The coefficients are:  [29.80066701 -0.31181577 -0.08955328]\n",
      "The mean squared error is:  66.13782635525362\n",
      "The root mean squared error is:  8.132516606515699\n",
      "The r2 score is:  0.2165579947742563\n",
      "The adjusted r2 score is:  0.21187607044023793\n",
      "The model has been fitted successfully with r squared: 0.2165579947742563\n"
     ]
    }
   ],
   "source": [
    "import main\n",
    "own_model = main.LinReg()\n",
    "own_model.fit(x=X, y=df[\"medv\"],verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5006662579253316e-11"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(own_model.residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "sk_model = LinearRegression()\n",
    "fitted = sk_model.fit(X,df[\"medv\"])\n",
    "print(fitted.coef_)\n",
    "print(fitted.intercept_)\n",
    "print(own_model.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.31181577 -0.08955328]\n",
      "29.800667010997643\n",
      "0.21655799477425608\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing for single independent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using OLS method to find the solution\n",
      "The model has been fitted successfully with OLS\n",
      "The coefficients are:  [24.03310617 -0.41519028]\n",
      "The mean squared error is:  71.69073588196659\n",
      "The root mean squared error is:  8.467038200100824\n",
      "The r2 score is:  0.15078046904975717\n",
      "The adjusted r2 score is:  0.14740385063643613\n",
      "The model has been fitted successfully\n"
     ]
    }
   ],
   "source": [
    "import main\n",
    "own_model_1d = main.LinReg()\n",
    "own_model_1d.fit(x=X[\"crim\"], y=df[\"medv\"],verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.41519028]\n",
      "24.03310617412388\n",
      "0.15078046904975717\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "sk_model_1d = LinearRegression()\n",
    "fitted_1d = sk_model_1d.fit(X[[\"crim\"]],df[\"medv\"])\n",
    "\n",
    "print(fitted_1d.coef_)\n",
    "print(fitted_1d.intercept_)\n",
    "print(sk_model_1d.score(X[[\"crim\"]],df[\"medv\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking with method \"GD\" for single variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import helper\n",
    "\n",
    "\n",
    "class LinReg:\n",
    "    \"\"\"\n",
    "    This class implements the linear regression model\n",
    "    Fit the model using .fit() method\n",
    "    Predict the target variable using .predict() method\n",
    "\n",
    "\n",
    "\n",
    "    After fitting the model, the following attributes are available:\n",
    "    self.beta: the coefficients of the model\n",
    "    self.residuals: the residuals of the model\n",
    "    self.mse: the mean squared error of the model\n",
    "    self.rmse: the root mean squared error of the model\n",
    "    self.r2: the r2 score of the model\n",
    "    self.r2_adj: the adjusted r2 score of the model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.y = None\n",
    "        self.x = None\n",
    "        self.beta = None\n",
    "        self.residuals = None\n",
    "        self._mse = None\n",
    "        self._rmse = None\n",
    "        self._r2 = None\n",
    "        self._r2_adj = None\n",
    "        self.y_hat = None\n",
    "        self.max_alpha = None\n",
    "        self.verbose = None\n",
    "        self.max_iter = None\n",
    "        self.min_alpha = None\n",
    "        self.history = {}\n",
    "        self.model_method = None\n",
    "        self.alpha = None\n",
    "        self.diff_w = None\n",
    "        self.xtx = None\n",
    "        self.xtx_inv = None\n",
    "        self.xty = None\n",
    "        self.intercept = None\n",
    "        self._y_mean = None\n",
    "        self._tss = None\n",
    "        self._aic = None\n",
    "        self._bic = None\n",
    "        # Add metaclass to return warning when user tries to access these variables before fitting the model\n",
    "        return\n",
    "\n",
    "    def fit(self, x, y, intercept=False, verbose=False, method=\"OLS\", alpha=0.01, max_iter=100):\n",
    "        \"\"\"\n",
    "        :param x: independent variables\n",
    "        :param y: target variable\n",
    "        :param method: the method to fit the model, OLS or GD\n",
    "                       Use GD for large data, function throws an error,\n",
    "                       if OLS is used on data with size > 50000\n",
    "        :param intercept: whether the intercept is pre-defined or not\n",
    "        :param verbose: Boolean, whether to print the summary of the model or not\n",
    "        :param alpha: learning rate for gradient descent - only applicable when method = GD\n",
    "        :param max_iter: maximum number of iterations - only applicable when method = GD\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # add the intercept term to x\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.verbose = verbose\n",
    "        self.max_iter = max_iter\n",
    "        self.alpha = alpha\n",
    "        self.intercept = intercept\n",
    "        self.method = method\n",
    "        self.x = helper.check_if_oned(\n",
    "            self.x)  # check if the independent variable is one dimensional, if so, reshapes it.\n",
    "        if self.method == \"OLS\":\n",
    "            if self.x.size < 50000:\n",
    "                return self.fit_ols(x=self.x, y=self.y, verbose=self.verbose)\n",
    "            else:\n",
    "                raise AssertionError(\"The data is too large for OLS method. Consider using gradient descent method\")\n",
    "        elif self.method == \"GD\":\n",
    "            if self.verbose:\n",
    "                print(\"Using gradient descent method to find the solution\")\n",
    "            return self.fit_gd(self.x, self.y,verbose = True)\n",
    "        elif self.method == \"MLE\":\n",
    "            if self.verbose:\n",
    "                print(\"Initializing\")\n",
    "            return self.fit_mle(self.x, self.y)\n",
    "\n",
    "    def fit_gd(self, x, y, verbose=False, alpha=0.001, max_iter=100, intercept=False, max_alpha=None,\n",
    "               min_alpha=None):\n",
    "        \"\"\"\n",
    "        calculates the coefficients using gradient descent method.\n",
    "\n",
    "        :param x:\n",
    "        :param y:\n",
    "        :param verbose:\n",
    "        :param alpha:\n",
    "        :param max_iter:\n",
    "        :param intercept:\n",
    "        :param max_alpha:\n",
    "        :param min_alpha:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # re-initialising self.x in case user uses fit_gd() directly\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.verbose = verbose\n",
    "        self.alpha = alpha\n",
    "        self.max_iter = max_iter\n",
    "        self.max_alpha = max_alpha\n",
    "        self.min_alpha = min_alpha\n",
    "        self.history = {}\n",
    "        self.model_method = \"GD\"\n",
    "        self.x = helper.check_if_oned(self.x)\n",
    "        self.x = helper.normalise(self.x)\n",
    "        self.y = helper.check_if_oned(self.y)\n",
    "        if not intercept:\n",
    "            self.x = helper.add_intercept(self.x)\n",
    "        if self.verbose:\n",
    "            print(\"Using gradient descent method to find the solution\")\n",
    "\n",
    "        self.beta = np.zeros((self.x.shape[1],1))\n",
    "\n",
    "        for i in range(self.max_iter):\n",
    "            \"\"\"if self.max_alpha is not None:\n",
    "                if self.max_iter > 0.8 * self.max_iter:\n",
    "                    self.alpha = self.min_alpha\n",
    "                else:\n",
    "                    self.alpha = self.max_alpha\"\"\"\n",
    "            \n",
    "            self.y_hat = np.dot(self.x, self.beta)\n",
    "            self.residuals = self.y - self.y_hat\n",
    "            # print(f\"residuals: {self.residuals}\")\n",
    "            self.diff_w = (-2 / len(x)) * np.dot(self.x.T, self.residuals)\n",
    "            self.beta = self.beta - (self.alpha * self.diff_w)\n",
    "            if i%100==0:\n",
    "                print(self.beta)\n",
    "            # bias is also updated. The intercept allows automatic calculation\n",
    "            #self.history.update(self._cal_metrics())\n",
    "\n",
    "        if self.verbose:\n",
    "            self._cal_metrics()\n",
    "            self.summary()\n",
    "            print(f\"The model has been fitted successfully with r squared: {self._r2}\")\n",
    "            return self\n",
    "        return self\n",
    "\n",
    "    def fit_mle(self, x, y, verbose=False, intercept=False):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.verbose = verbose\n",
    "        self.x = helper.check_if_oned(self.x)\n",
    "        if not self.x.shape[1] == 1:\n",
    "            return self.fit_ols(self.x, self.y, verbose=self.verbose, intercept=self.intercept)\n",
    "        else:\n",
    "            self.model_method = \"MLE\"\n",
    "            # calculating the beta\n",
    "            # Calculate the mean of X and y\n",
    "            if np.isnan(self.x).any():\n",
    "                raise ValueError(\"X contains NaN values. Please check your data\")\n",
    "            self.x_mean = np.mean(self.x)\n",
    "            self.y_mean = np.mean(self.y)\n",
    "            # print(self.x_mean, self.y_mean)\n",
    "            # Calculate the slope (coefficient)\n",
    "            numerator = np.sum((self.x - self.x_mean) * (self.y - self.y_mean))\n",
    "            denominator = np.sum((self.x - self.x_mean) ** 2)\n",
    "            # if denominator == 0:\n",
    "            #    raise ValueError(\"Denominator is zero. Check your data.\")\n",
    "            self.slope = (numerator / (denominator))\n",
    "            print(self.slope)\n",
    "            # Calculate the intercept\n",
    "            self.bias = self.y_mean - (self.slope * self.x_mean)\n",
    "            self.beta = [self.bias].extend(self.slope)\n",
    "            self._cal_metrics()\n",
    "            if self.verbose:\n",
    "                self.summary()\n",
    "                print(f\"The model has been fitted successfully with r squared: {self._r2}\")\n",
    "                return self\n",
    "            return self\n",
    "\n",
    "    def fit_ols(self, x, y, verbose=False, intercept=False):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.verbose = verbose\n",
    "        self.x = helper.check_if_oned(self.x)\n",
    "        if not self.intercept:\n",
    "            self.x = helper.add_intercept(self.x)\n",
    "        print(\"Using OLS method to find the solution\")\n",
    "        self.model_method = \"OLS\"\n",
    "        # calculating the beta\n",
    "        xtx = np.dot(self.x.T, self.x)\n",
    "        if np.linalg.det(xtx) == 0:\n",
    "            raise ValueError(\"Matrix X'X is singular and cannot be inverted.\")\n",
    "        xtx_inv = np.linalg.inv(xtx)\n",
    "        xty = np.dot(self.x.T, self.y)\n",
    "        self.beta = np.dot(xtx_inv, xty)\n",
    "        self._cal_metrics()\n",
    "        if self.verbose:\n",
    "            self.summary()\n",
    "            print(f\"The model has been fitted successfully with r squared: {self._r2}\")\n",
    "            return self\n",
    "        return self\n",
    "\n",
    "    def predict(self, x):\n",
    "        # sample prediction\n",
    "        pred = np.dot(self.x, self.beta)\n",
    "        return pred\n",
    "\n",
    "    def summary(self):\n",
    "        print(f\"The model has been fitted successfully with {self.model_method}\")\n",
    "        print(\"The coefficients are: \", self.beta)\n",
    "        # print(\"The residuals are: \", self.residuals)\n",
    "        print(\"The mean squared error is: \", self._mse)\n",
    "        print(\"The root mean squared error is: \", self._rmse)\n",
    "        print(\"The r2 score is: \", self._r2)\n",
    "        print(\"The adjusted r2 score is: \", self._r2_adj)\n",
    "        return\n",
    "\n",
    "    def _cal_metrics(self):\n",
    "        # Calculate metrics like MSE, RMSE, R-squared, adjusted R-squared, TSS, RSS, etc.\n",
    "        \n",
    "        if self.y_hat is None:\n",
    "            self.y_hat = np.dot(self.x, self.beta)\n",
    "        if self.residuals is None:\n",
    "            self.residuals = self.y - self.y_hat\n",
    "        self._calculate_mse()\n",
    "        self._calculate_rmse()\n",
    "        self._calculate_tss()\n",
    "        self._calculate_r2()\n",
    "        self._calculate_r2_adj()\n",
    "        self._calculate_aic()\n",
    "        self._calculate_bic()\n",
    "\n",
    "        return {'mse': self._mse, 'rmse': self._rmse}\n",
    "\n",
    "    def _calculate_mse(self):\n",
    "        if self._mse is None:\n",
    "            self._mse = np.mean(self.residuals ** 2)\n",
    "            return self._mse\n",
    "        return self._mse\n",
    "\n",
    "    def _calculate_rmse(self):\n",
    "        if self._rmse is None:\n",
    "            self._rmse = np.sqrt(self._mse)\n",
    "            return self._rmse\n",
    "        return self._rmse\n",
    "\n",
    "    def _calculate_r2(self):\n",
    "        if self._r2 is None:\n",
    "            self._r2 = 1 - (self._mse / self._tss)\n",
    "            return self._r2\n",
    "        return self._r2\n",
    "\n",
    "    def _calculate_tss(self):\n",
    "        if self._tss is None:\n",
    "            self._tss = np.sum((self.y - np.mean(self.y)) ** 2)\n",
    "            print(f\"tss: {self._tss}\")\n",
    "        return self._tss\n",
    "\n",
    "    def _calculate_r2_adj(self):\n",
    "        if self._r2_adj is None:\n",
    "            self._r2_adj = 1 - (1 - self._r2) * (len(self.y) - 1) / (len(self.y) - self.x.shape[1] - 1)\n",
    "        return self._r2_adj\n",
    "\n",
    "    def _calculate_aic(self):\n",
    "        if self._aic is None:\n",
    "            self._aic = helper.calculate_aic(len(self.x), self._mse, self.x.shape[1])\n",
    "        return self._aic\n",
    "\n",
    "    def _calculate_bic(self):\n",
    "        if self._bic is None:\n",
    "            self._bic = helper.calculate_bic(len(self.x), self._mse, self.x.shape[1])\n",
    "        return self._bic\n",
    "\n",
    "    def get_history(self):\n",
    "        if self.history == {}:\n",
    "            raise AssertionError(\"The model has not been fitted yet or the method is not GD\")\n",
    "        return self.history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_gd( x, y, verbose=False, alpha=0.01, max_iter=1000, intercept=False, max_alpha=None,\n",
    "            min_alpha=None):\n",
    "    \"\"\"\n",
    "    calculates the coefficients using gradient descent method.\n",
    "\n",
    "    :param x:\n",
    "    :param y:\n",
    "    :param verbose:\n",
    "    :param alpha:\n",
    "    :param max_iter:\n",
    "    :param intercept:\n",
    "    :param max_alpha:\n",
    "    :param min_alpha:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    def normalise(x):\n",
    "        for i in range(x.shape[1]):\n",
    "            print(f\"normalising {i}th column\")\n",
    "            x.iloc[:, i] = (x.iloc[:, i] - np.mean(x.iloc[:, i])) / np.std(x.iloc[:, i])\n",
    "        return x\n",
    "    # re-initialising x in case user uses fit_gd() directly\n",
    "    x = x\n",
    "    verbose = verbose\n",
    "    alpha = alpha\n",
    "    max_iter = max_iter\n",
    "    max_alpha = max_alpha\n",
    "    min_alpha = min_alpha\n",
    "    history = {}\n",
    "    model_method = \"GD\"\n",
    "    x = helper.check_if_oned(x)\n",
    "    x = normalise(x)\n",
    "    y = helper.check_if_oned(y)\n",
    "    if not intercept:\n",
    "        x = helper.add_intercept(x)\n",
    "    if verbose:\n",
    "        print(\"Using gradient descent method to find the solution\")\n",
    "\n",
    "    beta = np.zeros((x.shape[1],1))\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        \"\"\"if max_alpha is not None:\n",
    "            if max_iter > 0.8 * max_iter:\n",
    "                alpha = min_alpha\n",
    "            else:\n",
    "                alpha = max_alpha\"\"\"\n",
    "        \n",
    "        y_hat = np.dot(x, beta)\n",
    "        residuals = y - y_hat\n",
    "        # print(f\"residuals: {residuals}\")\n",
    "        diff_w = (-2 / len(x)) * np.dot(x.T, residuals)\n",
    "        beta = beta - (alpha * diff_w)\n",
    "        if i%100 == 0:\n",
    "            print(beta)\n",
    "        # bias is also updated. The intercept allows automatic calculation\n",
    "        #history.update(_cal_metrics())\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using gradient descent method to find the solution\n",
      "normalising 0th column\n",
      "normalising 1th column\n",
      "Using gradient descent method to find the solution\n",
      "[[ 0.04506561]\n",
      " [-0.00713549]\n",
      " [-0.00692693]]\n",
      "tss: medv    42716.295415\n",
      "dtype: float64\n",
      "The model has been fitted successfully with GD\n",
      "The coefficients are:  [[ 4.08819905]\n",
      " [-0.6265493 ]\n",
      " [-0.60697414]]\n",
      "The mean squared error is:  418.399833668844\n",
      "The root mean squared error is:  20.454824215056068\n",
      "The r2 score is:  medv    0.990205\n",
      "dtype: float64\n",
      "The adjusted r2 score is:  medv    0.990147\n",
      "dtype: float64\n",
      "The model has been fitted successfully with r squared: medv    0.990205\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.LinReg at 0x25ba705bdc0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import main\n",
    "own_model_gd2 = LinReg()\n",
    "own_model_gd2.fit(x=X, y=df[\"medv\"],verbose = True,method = \"GD\",max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.31181577 -0.08955328]\n",
      "29.800667010997643\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "sk_model = LinearRegression()\n",
    "fitted = sk_model.fit(X,df[\"medv\"])\n",
    "print(fitted.coef_)\n",
    "print(fitted.intercept_)\n",
    "#print(own_model.r2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
